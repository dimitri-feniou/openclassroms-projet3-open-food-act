{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Projet 3 Préparez des données pour un organisme de santé publique](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://www.hospitalia.fr/photo/art/grande/54117300-40801349.jpg?v=1614005985](https://www.hospitalia.fr/photo/art/grande/54117300-40801349.jpg?v=1614005985)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Présentation du projet](#toc0_)\n",
    "\n",
    "L'agence Santé publique France souhaite améliorer sa base de données **Open Food Facts**,base de données colaborative, libre et ouverte des produits alimentaire du monde entier. Elle contient notamment les informations de produits alimentaire (ingrédients,allergènes,valeurs nutritionnelles,qualité nutritionnelle (nutriscore)).\n",
    "\n",
    "- **Objectif du projet** :\n",
    "  - Nettoyage du jeu de données\n",
    "  - Analyse exploratoire du jeu de données\n",
    "    <br>\n",
    "- **Livrable attendu** :\n",
    "  1. Un notebook contenant l’ensemble de vos traitements des données ainsi que vos analyses\n",
    "  2. Un support de présentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>\n",
    "\n",
    "- [Projet 3 Préparez des données pour un organisme de santé publique](#toc1_)\n",
    "  - [Présentation du projet](#toc1_1_)\n",
    "  - [Importation des libraries python](#toc1_2_)\n",
    "  - [Importation du jeu de données](#toc1_3_)\n",
    "  - [Nettoyage des données](#toc1_4_)\n",
    "    - [Correction des types de variables](#toc1_4_1_)\n",
    "      - [Conversion des dates](#toc1_4_1_1_)\n",
    "      - [Conversion geocode](#toc1_4_1_2_)\n",
    "    - [Remplacement des valeurs manquantes non prises en compte](#toc1_4_2_)\n",
    "    - [Visualisation des valeurs manquantes](#toc1_4_3_)\n",
    "      - [Visualisation des valeurs manquantes Graphiques](#toc1_4_3_1_)\n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importation des libraries python](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:17:10.220684566Z",
     "start_time": "2023-12-08T15:17:10.208041916Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/3043887432.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mplotly\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpress\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmissingno\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mmsno\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.885930709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modification des affichages de colonnes, lignes et largeurs de colonnes pour avoir un maximum d'information\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Importation du jeu de données](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.886429375Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def import_dataset(dataframe: pd.DataFrame):\n",
    "    '''\n",
    "        Import the dataset openfoodfacts.\n",
    "        This function checks if the specified CSV file exists locally,create a folder called 'dataset' if it doesn't exist,downloads zip file containing the CSV file from remote URL if not found.\n",
    "        It then imports the data into the provided DataFrame.\n",
    "\n",
    "    Args :\n",
    "        dataframe (pd.DataFrame): Give a name to the Pandas DataFrame to store the imported CSV.\n",
    "\n",
    "    Return :\n",
    "        dataframe (pd.DataFrame): Return the Pandas DataFrame containing the imported CSV.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Specify the folder path and csv_file_path and too the download URL fir zip file download\n",
    "    folder_path = 'dataset'\n",
    "    download_csv = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/parcours-data-scientist/P2/fr.openfoodfacts.org.products.csv.zip'\n",
    "    csv_file_path = os.path.join(\n",
    "        folder_path, 'fr.openfoodfacts.org.products.csv')\n",
    "\n",
    "    # Check if the folder  'dataset' exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        # Create the folder if it doesn't exist\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Création du dossier: {folder_path}\")\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        # Specify the file path for the downloaded zip file\n",
    "        zip_file_path = os.path.join(\n",
    "            folder_path, 'fr.openfoodfacts.org.products.csv.zip')\n",
    "\n",
    "        # Download the zip file\n",
    "        urllib.request.urlretrieve(download_csv, zip_file_path)\n",
    "        print(f\"Téléchargement du fichier {download_csv}\")\n",
    "\n",
    "        # Uncompress the zip file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        print(\"Extraction du fichier CSV du zip\")\n",
    "\n",
    "        # Specify the file path for the extracted CSV file\n",
    "        csv_file_path = os.path.join(\n",
    "            folder_path, 'fr.openfoodfacts.org.products.csv')\n",
    "        print(f'Lecture du fichier dans le répertoire : {csv_file_path}')\n",
    "    else:\n",
    "        print(f'Lecture du fichier dans le répertoire : {csv_file_path}')\n",
    "    # Read the CSV file using pandas\n",
    "    # Create a list for specify values that treadted as missing values\n",
    "    na_values = [\"NA\", \"N/A\", \"NaN\", \"\", \"null\", \"None\", \".\"]\n",
    "    dataframe = pd.read_csv(csv_file_path, sep=\"\\t\",\n",
    "                            na_values=na_values, low_memory=False)\n",
    "    print(f\"CSV importé en dataframe via la variable df_food\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.887326834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the imported data\n",
    "df_food = pd.DataFrame()\n",
    "\n",
    "# Call the function and pass the empty DataFrame\n",
    "df_food = import_dataset(dataframe=df_food)\n",
    "\n",
    "df_food.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Nettoyage des données](#toc0_)\n",
    "\n",
    "Nous allons débuter notre exploration du jeu de donnée en une connaissance des variables présente et les comprendre pour déterminer les variables à garder pour la suite de notre analyse.<br>\n",
    "Pour mieux comprendre les champs disponible dans notre base de donnée un lien est [disponible ici](https://world.openfoodfacts.org/data/data-fields.txt).<br>\n",
    "Grâce a ce document on peut avoir une première catégorisation de nos champs :\n",
    "\n",
    "- Les informations générales sur la fiche du produit : nom, date de modification, etc.\n",
    "- Un ensemble de tags : catégorie du produit, localisation, origine, etc.\n",
    "- Les ingrédients composant les produits et leurs additifs éventuels.\n",
    "- Des informations nutritionnelles : quantité en grammes d’un nutriment pour 100 grammes du produit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.889162121Z"
    }
   },
   "outputs": [],
   "source": [
    "# See number of columns and rows on the dataset\n",
    "print('Nombre de lignes du dataset:',\n",
    "      df_food.shape[0], '\\nNombre de colonnes du dataset:', df_food.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.889562242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with columns dataframe\n",
    "list(df_food.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Correction des types de variables](#toc0_)\n",
    "\n",
    "Grâce à la description des colonnes sur le site on peu remarquer que certaines colonne on été convertie dans de mauvais types par pandas.Quand on transforme notre CSV en dataframe on le message suivant :\n",
    "\n",
    "```python\n",
    "DtypeWarning: Columns (0,3,5,19,20,24,25,26,27,28,35,36,37,38,39,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "```\n",
    "\n",
    "On va spécifier un type pour chaque colonne de notre dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.889922203Z"
    }
   },
   "outputs": [],
   "source": [
    "# See different data types in the column\n",
    "print('types de variable dans le dataset:\\n', df_food.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.890227270Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_types(csv_file_path: str):\n",
    "    \"\"\"\n",
    "    Convert data types of columns in a csv file.Need CSV file path to convert types.\n",
    "    We specify the different new type for each column.\\n\n",
    "    Args :\n",
    "        csv_file_path (str) : path of the CSV file to convert\n",
    "    returns :\n",
    "        dataframe (pd.DataFrame) : Return the Pandas Dataframe with new data type.\n",
    "        dtypes_columns : dictionary with column name and new data type for enter this variable on read_csv function\n",
    "        date_list : list with contain column name of column need to convert to datetime (we use this variable in other function).\n",
    "        specific_dtype : list with contain the column name with specific transform column (conversion geocode/we use this variable in other function).\n",
    "    \"\"\"\n",
    "    #\n",
    "    dtypes_columns = {}\n",
    "    date_list = []\n",
    "    change_type = []\n",
    "    specific_dtype = []\n",
    "\n",
    "\n",
    "# loop through the columns for attribute new data type\n",
    "    for column in (df_food.columns):\n",
    "        # dtypes object\n",
    "        if column in ['code', 'url', 'creator', 'product_name', 'generic_name', 'quantity', 'packaging', 'brands', 'origins', 'manufacturing_places', 'manufacturing_places_tags', 'packaging_tags', 'brands_tags', 'categories', 'categories_tags', 'categories_fr', 'origins_tags', 'labels', 'labels_tags', 'labels_fr', 'emb_codes', 'emb_codes_tags', 'cities', 'cities_tags', 'purchase_places', 'stores', 'countries', 'countries_tags', 'countries_fr', 'ingredients_text', 'allergens', 'allergens_fr', 'traces', 'traces_tags', 'traces_fr', 'serving_size', 'additives', 'additives_tags', 'additives_fr', 'ingredients_from_palm_oil_tags', 'ingredients_that_may_be_from_palm_oil_tags', 'states', 'states_tags', 'states_fr', 'main_category', 'main_category_fr', 'image_url', 'image_small_url']:\n",
    "            old_dtype = df_food[column].dtype\n",
    "            new_dtype = 'object'\n",
    "            dtypes_columns[column] = new_dtype\n",
    "            change_type.append(f\"{column}: {old_dtype} --> {new_dtype}\")\n",
    "            # dtypes float\n",
    "        elif column in ['ingredients_from_palm_oil_n', 'ingredients_from_palm_oil', 'ingredients_that_may_be_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil', 'no_nutriments', 'additives_n', 'energy_100g', 'energy-from-fat_100g', 'fat_100g', 'saturated-fat_100g', 'butyric-acid_100g', 'caproic-acid_100g', 'caprylic-acid_100g', 'capric-acid_100g', 'lauric-acid_100g', 'myristic-acid_100g', 'palmitic-acid_100g', 'stearic-acid_100g', 'arachidic-acid_100g', 'behenic-acid_100g', 'lignoceric-acid_100g', 'cerotic-acid_100g', 'montanic-acid_100g', 'melissic-acid_100g', 'monounsaturated-fat_100g', 'polyunsaturated-fat_100g', 'omega-3-fat_100g', 'alpha-linolenic-acid_100g', 'eicosapentaenoic-acid_100g', 'docosahexaenoic-acid_100g', 'omega-6-fat_100g', 'linoleic-acid_100g', 'arachidonic-acid_100g', 'gamma-linolenic-acid_100g', 'dihomo-gamma-linolenic-acid_100g', 'omega-9-fat_100g', 'oleic-acid_100g', 'elaidic-acid_100g', 'gondoic-acid_100g', 'mead-acid_100g', 'erucic-acid_100g', 'nervonic-acid_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g',\n",
    "                        'sucrose_100g', 'glucose_100g', 'fructose_100g', 'lactose_100g', 'maltose_100g', 'maltodextrins_100g', 'starch_100g', 'polyols_100g', 'fiber_100g', 'proteins_100g', 'casein_100g', 'serum-proteins_100g', 'nucleotides_100g', 'salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-a_100g', 'beta-carotene_100g', 'vitamin-d_100g', 'vitamin-e_100g', 'vitamin-k_100g', 'vitamin-c_100g', 'vitamin-b1_100g', 'vitamin-b2_100g', 'vitamin-pp_100g', 'vitamin-b6_100g', 'vitamin-b9_100g', 'folates_100g', 'vitamin-b12_100g', 'biotin_100g', 'pantothenic-acid_100g', 'silica_100g', 'bicarbonate_100g', 'potassium_100g', 'chloride_100g', 'calcium_100g', 'phosphorus_100g', 'iron_100g', 'magnesium_100g', 'zinc_100g', 'copper_100g', 'manganese_100g', 'fluoride_100g', 'selenium_100g', 'chromium_100g', 'molybdenum_100g', 'iodine_100g', 'caffeine_100g', 'taurine_100g', 'ph_100g', 'fruits-vegetables-nuts_100g', 'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g', 'carbon-footprint_100g', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g']:\n",
    "            old_dtype = df_food[column].dtype\n",
    "            new_dtype = 'float'\n",
    "            dtypes_columns[column] = new_dtype\n",
    "            change_type.append(f\"{column}: {old_dtype} --> {new_dtype}\")\n",
    "            # dtypes category\n",
    "        elif column in ['nutrition_grade_uk', 'nutrition_grade_fr', 'pnns_groups_1', 'pnns_groups_2']:\n",
    "            old_dtype = df_food[column].dtype\n",
    "            new_dtype = 'category'\n",
    "            dtypes_columns[column] = new_dtype\n",
    "            change_type.append(f\"{column}: {old_dtype} --> {new_dtype}\")\n",
    "            # dtypes int\n",
    "        elif column in []:\n",
    "            old_dtype = df_food[column].dtype\n",
    "            new_dtype = 'int'\n",
    "            dtypes_columns[column] = new_dtype\n",
    "            change_type.append(f\"{column}: {old_dtype} --> {new_dtype}\")\n",
    "            # dtypes datetime\n",
    "        elif column in ['created_t', 'created_datetime', 'last_modified_t', 'last_modified_datetime']:\n",
    "            old_dtype = df_food[column].dtype\n",
    "            new_dtype = 'datetime'\n",
    "            change_type.append(f\"{column}: {old_dtype} --> {new_dtype}\")\n",
    "            date_list.append(column)\n",
    "            # specific dtype\n",
    "        else:\n",
    "            specific_dtype.append(column)\n",
    "    # Apply read csv function with the new dtype conversion\n",
    "    df = pd.read_csv(\"dataset/fr.openfoodfacts.org.products.csv\",\n",
    "                     dtype=dtypes_columns, low_memory=False, sep=\"\\t\")\n",
    "    # Print changing types for each column with seperate lines\n",
    "    print(\"\\n\".join(change_type))\n",
    "    return df, date_list, specific_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.890483447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function to convert types, store date list and specific dtype\n",
    "df_food, date_list, specific_dtype = convert_types(\n",
    "    \"dataset/fr.openfoodfacts.org.products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[Conversion des dates](#toc0_)\n",
    "\n",
    "- Les dates qui termine \\_t sont des dates en UNIX timestamp format\n",
    "- Les dates qui termine \\_datetime sont des dates in the iso8601 format : yyyy-mm-ddThh:mn:ssZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.890764837Z"
    }
   },
   "outputs": [],
   "source": [
    "france_rows = df_food[df_food['created_t'] == 'France']\n",
    "\n",
    "display(france_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.891108735Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_date(date_list: list, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Conversion of date in datetime with specific format \n",
    "\n",
    "    Args :\n",
    "        date_list : list of date from columns dataframe\n",
    "        df : dataframe to convert\n",
    "    Returns :\n",
    "        dataframe with converted date\n",
    "    \"\"\"\n",
    "    for column in date_list:\n",
    "        if column.endswith(\"_t\"):\n",
    "            # Format as Unix timestamp (seconds since epoch)\n",
    "            print(column)\n",
    "            df[column] = pd.to_datetime(df[column], unit='s', errors='coerce')\n",
    "        elif column.endswith(\"_datetime\"):\n",
    "            # Format as ISO 8601 datetime\n",
    "            df[column] = pd.to_datetime(\n",
    "                df[column], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\n",
    "    # Display dataframe with only datetime columns\n",
    "    display(df[['created_t', 'created_datetime',\n",
    "            'last_modified_t', 'last_modified_datetime']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.891433497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the function convert_date\n",
    "df_food = convert_date(date_list, df_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_2_'></a>[Conversion geocode](#toc0_)\n",
    "\n",
    "La colonne 'first_packaging_code_geo' nous donne les cordonnées des packagings des produits.\n",
    "Nous allons supprimer cette colonne pour ajouter deux nouvelle colonne ayant la latitude et la longitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.891739401Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_food[['first_packaging_code_geo']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.925242785Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_geocode(specific_dtype: list, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create new columns for latitude and longitude from first_packaging_code_geo column\n",
    "    \"\"\"\n",
    "    for column in specific_dtype:\n",
    "        # Split the first_packaging_code_geo column into two columns\n",
    "        df[['fp_lat', 'fp_lon']] = df[column].str.split(\n",
    "            ',', n=1, expand=True)\n",
    "        # Convert fp_lat and fp_lon to float\n",
    "        df.fp_lat = round(df.fp_lat.astype(float), 2)\n",
    "        df.fp_lon = round(df.fp_lon.astype(float), 2)\n",
    "        # drop first_packaging_code_geo column\n",
    "        df = df.drop(columns=\"first_packaging_code_geo\")\n",
    "        display(df[['fp_lat', 'fp_lon']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.925632360Z"
    }
   },
   "outputs": [],
   "source": [
    "df_food = convert_geocode(specific_dtype, df_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Remplacement des valeurs manquantes non prises en compte](#toc0_)\n",
    "\n",
    "Il existe un grand nombre de valeur manquante dans notre dataframe nous allons vérifier si il n'y a pas d'autres valeurs manquantes. On sait que les valeurs manquantes sont comptabiliser par pandas avant les valeurs suivantes :NaN, None, or NaT. Vérifions maintenant qu'il n'existe pas d'autres valeurs manquantes et remplaçons les.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.925938753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to replace missing values\n",
    "def replace_missing_values(dataframe: pd.DataFrame, missing_values=['N/A', '-', 'Nan', 'nan']):\n",
    "    missing_count = dataframe.isna().sum().sum()\n",
    "    print(f\"Number of Missing Values: {missing_count}\")\n",
    "    for col in dataframe.columns:\n",
    "        dataframe[col] = dataframe[col].replace(missing_values, np.nan)\n",
    "    print(f\"Number of Missing Values: {missing_count}\")\n",
    "    return dataframe, missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.926294943Z"
    }
   },
   "outputs": [],
   "source": [
    "df, missing_count = replace_missing_values(\n",
    "    df_food, missing_values=['N/A', '-', 'Nan', 'nan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des valeurs dupliqués\n",
    "\n",
    "Avec les informations que l'on dispose on sait de la colonne code représente le code de chaque produit et doit donc être unique pour chaque produit dans notre dataframe. Vérifions si il existe des produits doublonnés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.926726073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify if\n",
    "# Choose the column for which you want to count unique values\n",
    "selected_column = 'code'\n",
    "\n",
    "# Get the number of unique values in the selected column\n",
    "unique_values = df_food[selected_column].nunique()\n",
    "\n",
    "# Get the total number of values in the selected column\n",
    "total_values = len(df_food[selected_column])\n",
    "\n",
    "# Calculate the difference\n",
    "difference = total_values - unique_values\n",
    "# Print unique/non unique values in the column\n",
    "print(f\"Nombre de valeurs uniques dans '{selected_column}': {unique_values}\")\n",
    "print(f\"Nombre de valeurs égales: {difference}\")\n",
    "# Display non-unique values in the 'code' column\n",
    "non_unique_values = df_food[df_food.duplicated(\n",
    "    subset=[selected_column], keep=False)]\n",
    "print(f\"valeur non-unique dans la colonne '{selected_column}':\")\n",
    "print(non_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.927007533Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_missing_values_column(dataframe: pd.DataFrame, column_name: str):\n",
    "    \"\"\"\n",
    "    Drop rows with missing values in a specific column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column in which to drop missing values.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with rows containing missing values in the specified column removed.\n",
    "    int: The number of rows before dropping missing values.\n",
    "    int: The number of rows after dropping missing values.\n",
    "    \"\"\"\n",
    "    # Store the number of rows before dropping missing values\n",
    "    original_rows = len(dataframe)\n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = dataframe.copy()\n",
    "\n",
    "    # Drop rows with missing values in the specified column\n",
    "    df_copy.dropna(subset=[column_name], inplace=True)\n",
    "\n",
    "    # Store the number of rows after dropping missing values\n",
    "    modified_rows = len(df_copy)\n",
    "    # Check if rows have been delete in the dataframe\n",
    "    print(\n",
    "        f\"Nombre de ligne dans le DataFrame avant suppression: {original_rows}\")\n",
    "    print(\n",
    "        f\"Nombre de ligne dans le DataFrame après suppression: {modified_rows}\")\n",
    "    return df_copy, original_rows, modified_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.927251628Z"
    }
   },
   "outputs": [],
   "source": [
    "df_food, original_rows, modified_rows = drop_missing_values_column(\n",
    "    df_food, 'code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons pu vérifier qu'il n'existe que des valeurs unique dans cette colonne, nous avons simplement supprimer les valeurs manquantes dans cette colonne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[Visualisation des valeurs manquantes](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons ensemble le taux de complétion de notre dataframe par colonnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.927477844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to see the completion of dataframe per column\n",
    "def missing_value_dataframe(dataframe: pd.DataFrame):\n",
    "    \"\"\"Function for calculating missing values per column to dataframe. \n",
    "    Get the percentage of non-missing and total missing values per column\n",
    "\n",
    "    parameters :\n",
    "        dataframe : pandas dataframe to calculate missing values\n",
    "    return :\n",
    "    Specific dataframe with missing values per column   \n",
    "    \"\"\"\n",
    "    # Count missing values per column\n",
    "    missing_count = dataframe.isnull().sum()\n",
    "\n",
    "    # Count non-missing values per column\n",
    "    non_missing_count = dataframe.notnull().sum()\n",
    "\n",
    "    # Calculate the total number of values per column\n",
    "    total_count = len(dataframe)\n",
    "\n",
    "    # Calculate the percentage of non-missing values\n",
    "    percentage_non_missing = (non_missing_count / total_count) * 100\n",
    "\n",
    "    # Create a new DataFrame with the values\n",
    "    df_missing_values = pd.DataFrame({\n",
    "        'Taux de remplissage': percentage_non_missing,\n",
    "        'Nombre de valeurs manquantes': missing_count})\n",
    "    df_missing_values = df_missing_values.sort_values(\n",
    "        by='Taux de remplissage', ascending=False)\n",
    "\n",
    "    return df_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:07.927722916Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_value_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1177707950.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_missing_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmissing_value_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_food\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_missing_values\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'missing_value_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "df_missing_values = missing_value_dataframe(df_food)\n",
    "display(df_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_3_1_'></a>[Visualisation des valeurs manquantes Graphiques](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.343718355Z",
     "start_time": "2023-12-08T15:03:07.983107480Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'msno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/3523145420.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Use missingno to visualize missing data with graphic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmsno\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_food\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msort\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ascending\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"dodgerblue\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'msno' is not defined"
     ]
    }
   ],
   "source": [
    "# Use missingno to visualize missing data with graphic\n",
    "msno.bar(df_food, sort=\"ascending\", color=\"dodgerblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:08.254957659Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_food' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1058077670.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Calculate the count of missing values and non-missing values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmissing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_food\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mnon_missing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_food\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Create a pie chart for see the representation of dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m fig = px.pie(\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_food' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the count of missing values and non-missing values\n",
    "missing_count = df_food.isna().sum().sum()\n",
    "non_missing_count = df_food.count().sum()\n",
    "# Create a pie chart for see the representation of dataframe\n",
    "fig = px.pie(\n",
    "    names=['Valeurs manquantes', 'Valeurs non-manquantes'],\n",
    "    values=[missing_count, non_missing_count],\n",
    "    title=\"Représentation des valeurs manquantes/non manquantes dataframe\",\n",
    ")\n",
    "# Show the pie chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.488773227Z",
     "start_time": "2023-12-08T15:03:08.364044171Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_missing_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/906149656.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;31m# Apply the categorize_percentage function to the DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m df_missing_values['Pourcentage_category'] = df_missing_values['Taux de remplissage'].apply(\n\u001B[0m\u001B[1;32m     24\u001B[0m     categorize_percentage)\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_missing_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a chart pie for group column in different pourcentage\n",
    "\n",
    "def categorize_percentage(percentage):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if percentage == 100:\n",
    "        return \"100%\"\n",
    "    elif percentage < 100 and percentage >= 90:\n",
    "        return \"99-90%\"\n",
    "    elif percentage >= 50 and percentage < 90:\n",
    "        return \"89-50%\"\n",
    "    elif percentage <= 50 and percentage >= 10:\n",
    "        return \"50-10%\"\n",
    "    elif percentage <= 10 and percentage >= 1:\n",
    "        return \"10-01%\"\n",
    "    elif percentage <= 1 and percentage > 0.0:\n",
    "        return \"1-0.01%\"\n",
    "    else:\n",
    "        return \"0%\"\n",
    "\n",
    "\n",
    "# Apply the categorize_percentage function to the DataFrame\n",
    "df_missing_values['Pourcentage_category'] = df_missing_values['Taux de remplissage'].apply(\n",
    "    categorize_percentage)\n",
    "\n",
    "# Group and count the percentages by category\n",
    "category_counts = df_missing_values['Pourcentage_category'].value_counts(\n",
    ").reset_index()\n",
    "category_counts.columns = ['Pourcentage_category', 'Count']\n",
    "\n",
    "# Create a pie chart with the grouped percentages\n",
    "fig = px.pie(category_counts, names='Pourcentage_category',\n",
    "             values='Count', title='Nombre de colonne par pourcentage de complétion')\n",
    "# Add a title to the legend\n",
    "fig.update_layout(legend_title_text='Groupement des colonnes par pourcentage')\n",
    "# Show the pie chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obervation des valeurs manquantes dans notre dataframe\n",
    "\n",
    "- **76 % de notre dataframe contient des valeurs manquantes**\n",
    "- **3% de notre dataframe ne contient aucune valeurs manquantes**, soit 5/162 colonnes\n",
    "- **10% de notre dataframe a taux de complétion compris entre 100% et 90%**, soit 16/162 colonnes\n",
    "- **11% de notre dataframe a taux de complétion compris entre 89% et 50%**, soit 18/162 colonnes\n",
    "- **79% de notre dataframe a taux de complétion inférieur a 50%**, soit 130/162 colonnes\n",
    "- **13% de notre dataframe ne contient que des valeurs nulle**, soit 21/162 colonnes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons d'information sur les valeurs manquantes dans notre dataframe, nous allons sélectionner les colonnes en fonction de leurs pertinance pour notre analyse ainsi qu'en fonction de la complétion dans le dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode de calcul du Nutri-Score\n",
    "\n",
    "On sait que la valeurs la plus importantes dans notre jeux de données est le _Nutri-Score_.\n",
    "Mais comment cette valeurs est t-elle calculé ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quesce-ce que le Nutri-Score ?\n",
    "\n",
    "\"C'est une note permettant d'avoir une information sur la qualité nutritionnelle globale d'un produit alimentaire. Son but est d'aider les consommateurs au choix d'aliments sains pour sa santé.\n",
    "\n",
    "Elle se base sur les travaux de l’équipe du Professeur Serge Hercberg (Université Paris 13), les expertises de l’Anses et du Haut Conseil de Santé Publique.\"[(Source)]('https://simonetthomas.github.io/CalculateurNutriscore/faq.html#nutriscore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment est-il calculé ?\n",
    "\n",
    "![nutriscore_image](https://assets-global.website-files.com/5e4d08c15370e9073c634a54/62e3febf1c959db9cb85c2a6_le-nutri-score-debarque-chez-foodles-calcul-food-in-action.jpeg)\n",
    "\n",
    "\"La méthode de calcul est basée sur l'assignation de points en fonction de la valeur de chaque nutriment considéré comme « à éviter » ou « à favoriser ». Les valeurs sur lesquelles on se base sont celles indiquées dans la « déclaration nutritionnelle obligatoire », souvent présent sous la forme d'un tableau à l'arrière du produit.\n",
    "\n",
    "Les éléments à éviter sont ceux dont les points sont indiqués en rouge dans le tableau : apport calorique, acides gras saturés, sucres, et sodium (calculé d'après le sel). Les éléments à favoriser sont indiqués en vert : teneur en fibres, protéines, et en fruits, légumes et noix.\n",
    "\n",
    "La règle générale est de soustraire ensuite ces deux sous-totaux de points (points négatifs - points positifs), résultant en un score nutritionnel, le plus faible étant le meilleur. Une note de A à E est ensuite déterminée sur la base de ce score, A étant la meilleure, et E la plus mauvaise.\"[(Source)]('https://simonetthomas.github.io/CalculateurNutriscore/faq.html#nutriscore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation :\n",
    "- Dans le calcul du nutri-score voici les informations nutritionnelles qui sont pris en compte :\n",
    "  - Energie (kJ)\n",
    "  - Acides gras saturés (g)\n",
    "  - Sucres (g)\n",
    "  - Fibres (g)\n",
    "  - Protéines (g)\n",
    "  - Sel (g)\n",
    "  - Sodium (g)\n",
    "  - Fruits,légumes et noix (%) calcul des ingrédients représentant des fruits,légumes, ou noix dans la liste d'ingrédients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrage de notre dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choix des colonnes :\n",
    "\n",
    "  - Information Nutritionnel présent dans le calcul du nutri-score ('energy_100g', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g'). Non garderons pas le 'sodium' car c'est un ingrédients pour le sel.\n",
    "  - Informations générale sur le produit ('code', 'url', 'product_name', 'brands', 'countries_fr')\n",
    "  - Information annexes sur le produit ('additives_fr', 'quantity', 'pnns_groups_1', 'pnns_groups_2', 'ingredients_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.552097407Z",
     "start_time": "2023-12-08T15:03:08.455878928Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_dataframe(dataframe, columns_to_select):\n",
    "    \"\"\"\n",
    "    Select specific columns from a DataFrame and create a new DataFrame with those columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: The original DataFrame.\n",
    "    - columns_to_select: A list of column names to select.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame containing only the selected columns.\n",
    "    \"\"\"\n",
    "    if not set(columns_to_select).issubset(dataframe.columns):\n",
    "        # Check if all columns_to_select are present in the original DataFrame\n",
    "        missing_columns = set(columns_to_select) - set(dataframe.columns)\n",
    "        raise ValueError(\n",
    "            f\"Columns not found in the DataFrame: {missing_columns}\")\n",
    "\n",
    "    dataframe = dataframe[columns_to_select].copy()\n",
    "    # Conversion factor from kJ to kcal\n",
    "    conversion_factor = 0.239006\n",
    "\n",
    "    # Create a new column 'energy_100g_kcal' with energy values in kcal\n",
    "    dataframe['energy_100g_kcal'] = dataframe['energy_100g'] * \\\n",
    "        conversion_factor\n",
    "    # Drop the original 'energy_100g' column to keep energy_100g_kcal\n",
    "    dataframe.drop(columns=['energy_100g'], inplace=True)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.583514146Z",
     "start_time": "2023-12-08T15:03:08.509788375Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_food' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/136181157.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Use function filter_dataframe for select columns in dataframe **df_food**\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m df_filtered = filter_dataframe(df_food, ['code', 'url', 'product_name', 'brands', 'countries_fr', 'nutrition-score-fr_100g', 'nutrition_grade_fr',\n\u001B[0m\u001B[1;32m      4\u001B[0m                                'additives_n', 'additives_fr', 'quantity', 'energy_100g', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g'])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_food' is not defined"
     ]
    }
   ],
   "source": [
    "# Use function filter_dataframe for select columns in dataframe **df_food**\n",
    "\n",
    "df_filtered = filter_dataframe(df_food, ['code', 'url', 'product_name', 'brands', 'countries_fr', 'nutrition-score-fr_100g', 'nutrition_grade_fr',\n",
    "                               'additives_n', 'additives_fr', 'quantity', 'energy_100g', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyage des données filtrés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nettoyage de la colonne **countries_fr**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:08.575466553Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/838047418.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_filtered\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'countries_fr'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "df_filtered['countries_fr'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.661539271Z",
     "start_time": "2023-12-08T15:03:08.632669851Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1829689551.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;31m# Call the function to drop rows not containing 'France'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m df_filtered = drop_rows_not_containing_substring(\n\u001B[0;32m---> 23\u001B[0;31m     df_filtered, 'countries_fr', 'France')\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def drop_rows_not_containing_substring(dataframe, column_name, substring):\n",
    "    # Create a boolean mask to identify rows where the specified column does not contain the substring\n",
    "    mask = dataframe[column_name].str.contains(substring, case=False, na=False)\n",
    "\n",
    "    # Use the mask to filter the DataFrame, keeping only the rows that match the condition\n",
    "    dataframe = dataframe[mask]\n",
    "    # Get the number of rows in each dataframe\n",
    "    number_rows_old_df = len(df_food)\n",
    "    number_rows_new_df = len(dataframe)\n",
    "    difference_num_rows = number_rows_old_df - number_rows_new_df\n",
    "\n",
    "    # Print the number of rows\n",
    "    print(\n",
    "        f\"Nombre de lignes dans le dataframe n'ayant pas le mot france: {difference_num_rows}\")\n",
    "    print(\n",
    "        f'Nombre de lignes dans le dataframe contenant uniquement des produits vendu en France: {number_rows_new_df}')\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Call the function to drop rows not containing 'France'\n",
    "df_filtered = drop_rows_not_containing_substring(\n",
    "    df_filtered, 'countries_fr', 'France')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que notre dataset contient uniquement des données de produit vendues en France,regardons les données manquantes dans nos données filtrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:08.657463618Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_value_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/2613279135.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmissing_value_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_filtered\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'missing_value_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "missing_value_dataframe(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.813508003Z",
     "start_time": "2023-12-08T15:03:08.717095853Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1688244398.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Calculate the percentage of rows with missing values in both columns A and B\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m percentage_missing_values_both = (df_filtered['product_name'].isnull(\n\u001B[0m\u001B[1;32m      3\u001B[0m ) & df_filtered['brands'].isnull()).sum() / len(df_filtered) * 100\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Display the result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the percentage of rows with missing values in both columns A and B\n",
    "percentage_missing_values_both = (df_filtered['product_name'].isnull(\n",
    ") & df_filtered['brands'].isnull()).sum() / len(df_filtered) * 100\n",
    "\n",
    "# Display the result\n",
    "print(\n",
    "    f\"Percentage of rows with missing values in both columns product_name and brands: {percentage_missing_values_both:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.831287059Z",
     "start_time": "2023-12-08T15:03:08.800721685Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_missing_values(dataframe, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drops rows with missing values in specified columns from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - columns_to_drop (list): List of column names where missing values should be dropped.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: .\n",
    "    \"\"\"\n",
    "    df_dropped = dataframe.dropna(subset=columns_to_drop)\n",
    "    dataframe = missing_value_dataframe(df_dropped)\n",
    "    print(\n",
    "        f'Suppression des valeurs manquantes dans les colonnes {columns_to_drop}')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.907243907Z",
     "start_time": "2023-12-08T15:03:08.817472958Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/3909959565.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Call the function for drop columns with missing values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdrop_missing_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_filtered\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'product_name'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'brands'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the function for drop columns with missing values\n",
    "drop_missing_values(df_filtered, ['product_name', 'brands'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement des données manquantes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les types de données manquantes\n",
    "\n",
    "Avant le traitement des données manquantes par différentes méthodes d'imputation,nous devons identifier que type de données manquantes nous avons dans nos échantillons de données.\n",
    "Il existe différent types de données manquantes :\n",
    "\n",
    "- **MCAR** (Missing Completely at Random) Une donnée est MCAR, c’est-à-dire manquante de façon complètement aléatoire si la probabilité d’absence est la même pour toutes les observations. Cette probabilité ne dépend donc que de paramètres extérieurs indépendants de cette variable\n",
    "- **MAR** (Missing At Random) les valeurs manquantes dépendent des autres données observées, mais pas des données manquantes\n",
    "- **MNAR** (Missing Not At Random) La donnée est manquante de façon non aléatoire si la probabilité d’absence dépend de la variable en question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation des données manquantes de nos données filtrés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:08.884628418Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_value_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/2613279135.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmissing_value_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_filtered\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'missing_value_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "missing_value_dataframe(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification des types de données manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:08.999754587Z",
     "start_time": "2023-12-08T15:03:08.914420370Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'msno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/3860322302.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmsno\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheatmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_filtered\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'msno' is not defined"
     ]
    }
   ],
   "source": [
    "msno.heatmap(df_filtered, labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:08.981137716Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'msno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/962595985.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmsno\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdendrogram\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_filtered\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'msno' is not defined"
     ]
    }
   ],
   "source": [
    "msno.dendrogram(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce aux visualisation graphique de nos données manquantes, on peut remarquer que nos données sont fortement liées aux autre variables manquantes, on a donc dans notre jeux de données filtrer des données manquantes de types **MAR**. Nous allons Imputer nos données via la méthode **KNN Inputer** pour nos données numérique et pour nos données catégorielle nous allons utiliser le **SimpleImputer** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Imputer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.072549515Z",
     "start_time": "2023-12-08T15:03:09.048654255Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "# Skip this cell when running notebook\n",
    "%%skip\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def optimize_k(data, target, categorical_cols):\n",
    "    errors = []\n",
    "    for k in range(1, 30):\n",
    "\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        df_imputed = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "        X = df_imputed.drop(target, axis=1)\n",
    "        y = df_imputed[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = rmse(y_test, preds)\n",
    "        errors.append({'K': k, 'RMSE': error})\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_filtered.select_dtypes(include=[np.number])\n",
    "categorical_cols = df_filtered.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Specify the target column\n",
    "target_column = 'nutrition-score-fr_100g'\n",
    "\n",
    "\n",
    "# Call the optimization function\n",
    "k_errors = optimize_k(data=numerical_cols,\n",
    "                      target=target_column, categorical_cols=categorical_cols)\n",
    "\n",
    "# Print or analyze the results\n",
    "print(k_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.071831547Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/990777083.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mk_rmse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'K'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'RMSE'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_trace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mScatter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'K'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'RMSE'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m fig.update_layout(title='RMSE pour les différentes valeurs de K',\n",
      "\u001B[0;31mNameError\u001B[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "k_rmse = [{'K': 1, 'RMSE': 2.584246879542272}, {'K': 2, 'RMSE': 2.592634459053153}, {'K': 3, 'RMSE': 2.6557456621641498}, {'K': 4, 'RMSE': 2.7157149608094495}, {'K': 5, 'RMSE': 2.6827404859081887}, {'K': 6, 'RMSE': 2.651040719334745}, {'K': 7, 'RMSE': 2.6679174745720777}, {'K': 8, 'RMSE': 2.6536641153484988}, {'K': 9, 'RMSE': 2.654174432555565}, {'K': 10, 'RMSE': 2.632810139669687}, {'K': 11, 'RMSE': 2.6053500206147437}, {'K': 12, 'RMSE': 2.6093688853118984}, {'K': 13, 'RMSE': 2.589366016342716}, {'K': 14, 'RMSE': 2.5959565924885717}, {\n",
    "    'K': 15, 'RMSE': 2.591174671693854}, {'K': 16, 'RMSE': 2.5683151864038667}, {'K': 17, 'RMSE': 2.55483794464236}, {'K': 18, 'RMSE': 2.5388365709891048}, {'K': 19, 'RMSE': 2.527759984443189}, {'K': 20, 'RMSE': 2.5261167705241867}, {'K': 21, 'RMSE': 2.522365320439682}, {'K': 22, 'RMSE': 2.5339303756037523}, {'K': 23, 'RMSE': 2.5134252907793577}, {'K': 24, 'RMSE': 2.5031421089065082}, {'K': 25, 'RMSE': 2.520884348637867}, {'K': 26, 'RMSE': 2.5120377513343772}, {'K': 27, 'RMSE': 2.506308660617449}, {'K': 28, 'RMSE': 2.4953048476194835}, {'K': 29, 'RMSE': 2.485284666373997}]\n",
    "\n",
    "results = pd.DataFrame(data=k_rmse, columns=['K', 'RMSE'])\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=results['K'], y=results['RMSE']))\n",
    "fig.update_layout(title='RMSE pour les différentes valeurs de K',\n",
    "                  xaxis_title='k', yaxis_title='Root Mean Squared Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.157535224Z",
     "start_time": "2023-12-08T15:03:09.128672160Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/4086396601.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m columns_with_missing_values = df_filtered.columns[df_filtered.isnull(\n\u001B[0m\u001B[1;32m      2\u001B[0m ).any()].tolist()\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdf_missing_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_filtered\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcolumns_with_missing_values\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_with_missing_values = df_filtered.columns[df_filtered.isnull(\n",
    ").any()].tolist()\n",
    "\n",
    "df_missing_values = df_filtered[columns_with_missing_values]\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_missing_values.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df_missing_values.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Impute numerical columns using KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=29)\n",
    "df_numerical_imputed = pd.DataFrame(imputer.fit_transform(\n",
    "    df_missing_values[numerical_cols]), columns=numerical_cols)\n",
    "\n",
    "# Impute categorical columns using SimpleImputer with 'most_frequent' strategy\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "df_categorical_imputed = pd.DataFrame(categorical_imputer.fit_transform(\n",
    "    df_missing_values[categorical_cols]), columns=categorical_cols)\n",
    "\n",
    "\n",
    "# Concatenate the results\n",
    "df_imputed = pd.concat([df_numerical_imputed, df_categorical_imputed], axis=1)\n",
    "\n",
    "print(\"Imputed DataFrame:\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.152405178Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_value_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/132108759.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmissing_value_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_imputed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'missing_value_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "missing_value_dataframe(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détection des outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparaison de nos données filtrées et données imputées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.254768287Z",
     "start_time": "2023-12-08T15:03:09.216760076Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/623078713.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_filtered\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdescribe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'df_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.254341488Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/200633118.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_imputed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdescribe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "df_imputed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idenfication graphique des outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.440043730Z",
     "start_time": "2023-12-08T15:03:09.335044744Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1108749130.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Select numeric value from df_imputed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m df_outlier_graph_nutri = df_imputed[['proteins_100g',\n\u001B[0m\u001B[1;32m      3\u001B[0m                                      'sugars_100g', 'fiber_100g', 'salt_100g', 'nutrition-score-fr_100g', 'additives_n']]\n\u001B[1;32m      4\u001B[0m fig = make_subplots(rows=1, cols=len(\n\u001B[1;32m      5\u001B[0m     df_outlier_graph_nutri.columns), shared_yaxes=True)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "# Select numeric value from df_imputed\n",
    "df_outlier_graph_nutri = df_imputed[['proteins_100g',\n",
    "                                     'sugars_100g', 'fiber_100g', 'salt_100g', 'nutrition-score-fr_100g', 'additives_n']]\n",
    "fig = make_subplots(rows=1, cols=len(\n",
    "    df_outlier_graph_nutri.columns), shared_yaxes=True)\n",
    "\n",
    "# Iterate through each column and create a violin plot\n",
    "for i, col in enumerate(df_outlier_graph_nutri.columns, start=1):\n",
    "    trace = go.Violin(y=df_outlier_graph_nutri[col], name=col)\n",
    "    fig.add_trace(trace, row=1, col=i)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Distribution des informations nutritionnelles\", showlegend=False)\n",
    "fig.update_yaxes(title_text=\"Valeurs de chaque colonne\", row=1, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification des outliers avec IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.412629870Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/2682729147.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'nutrition-score-fr_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'energy_100g_kcal'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'proteins_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'sugars_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'fiber_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'salt_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'additives_n'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0;31m# use the function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m     \u001B[0mindex_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdetect_outliers_iqr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_imputed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Outliers détecter pour la colonne {feature}: {len(index_list)}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0;31m# See rows with outliers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "# Function for detect outliers with IQR Method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data.index[(data[column] < lower_bound) | (\n",
    "        data[column] > upper_bound)]\n",
    "    return data.index[(data[column] < lower_bound) | (\n",
    "        data[column] > upper_bound)]\n",
    "\n",
    "\n",
    "# Create an empty list to store the output indices from multiple columns\n",
    "index_list = []\n",
    "for feature in ['nutrition-score-fr_100g', 'energy_100g_kcal', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']:\n",
    "    # use the function\n",
    "    index_list.extend(detect_outliers_iqr(df_imputed, feature))\n",
    "    print(f\"Outliers détecter pour la colonne {feature}: {len(index_list)}\")\n",
    "    # See rows with outliers\n",
    "\n",
    "display(df_imputed.iloc[index_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.535231982Z",
     "start_time": "2023-12-08T15:03:09.447406878Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1370490849.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;31m# Example usage:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m outliers_info = detect_outliers_iqr(df_imputed, [\n\u001B[0m\u001B[1;32m     26\u001B[0m                                     'nutrition-score-fr_100g', 'energy_100g_kcal', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n'])\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "def detect_outliers_iqr(data, features):\n",
    "    outliers = pd.DataFrame(index=data.index)\n",
    "\n",
    "    for feature in features:\n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create a mask for outliers for the current feature\n",
    "        outlier_mask = (data[feature] < lower_bound) | (\n",
    "            data[feature] > upper_bound)\n",
    "\n",
    "        # Add a new column indicating whether the feature is an outlier\n",
    "        outliers[feature + '_outlier'] = outlier_mask.astype(int)\n",
    "\n",
    "    # Sum the outlier columns to count the total number of outlier features for each row\n",
    "    outliers['total_outliers'] = outliers.sum(axis=1)\n",
    "\n",
    "    return outliers[outliers['total_outliers'] > 0]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "outliers_info = detect_outliers_iqr(df_imputed, [\n",
    "                                    'nutrition-score-fr_100g', 'energy_100g_kcal', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n'])\n",
    "\n",
    "# Display the DataFrame with information about outlier features\n",
    "display(outliers_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Représentation graphique des outliers avec la méthodes IQR pour chaque colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.524404500Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/4261007727.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mplotly\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpress\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Function for detecting outliers with IQR Method\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Function for detecting outliers with IQR Method\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(data, features):\n",
    "    outliers = pd.DataFrame(index=data.index)\n",
    "\n",
    "    for feature in features:\n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create a mask for outliers for the current feature\n",
    "        outlier_mask = (data[feature] < lower_bound) | (\n",
    "            data[feature] > upper_bound)\n",
    "\n",
    "        # Add a new column indicating whether the feature is an outlier\n",
    "        outliers[feature + '_outlier'] = outlier_mask.astype(int)\n",
    "\n",
    "    # Sum the outlier columns to count the total number of outlier features for each row\n",
    "    outliers['total_outliers'] = outliers.sum(axis=1)\n",
    "\n",
    "    return outliers[outliers['total_outliers'] > 0]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df_imputed is your DataFrame\n",
    "features_to_check = ['nutrition-score-fr_100g', 'energy_100g_kcal',\n",
    "                     'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']\n",
    "outliers_info = detect_outliers_iqr(df_imputed, features_to_check)\n",
    "\n",
    "# Concatenate outliers_info with df_imputed\n",
    "df_with_outliers_info = pd.concat([df_imputed, outliers_info], axis=1)\n",
    "df_with_outliers_info = df_with_outliers_info.fillna(0)\n",
    "\n",
    "# Create visualizations for each feature\n",
    "for feature in features_to_check:\n",
    "    fig = px.scatter(df_with_outliers_info, x=feature, color=feature+'_outlier',\n",
    "                     labels={feature+'_outlier': f'Outlier ({feature})'},\n",
    "                     title=f'Outliers Visualization for {feature}')\n",
    "    fig.update_layout(title_text=f'Outliers Visualization for {feature}')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idenfication des outliers avec le Z-scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.611493198Z",
     "start_time": "2023-12-08T15:03:09.584647776Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/3338666672.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mindex_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# Calculate the total number of rows in the DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mtotal_rows\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_imputed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'nutrition-score-fr_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'energy_100g_kcal'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'proteins_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'sugars_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'fiber_100g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'salt_100g'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "# Function for detect outliers with Z-score Method\n",
    "def detect_outliers_zscore(data, column, threshold=2):\n",
    "    z_scores = (data[column] - data[column].mean()) / data[column].std()\n",
    "    return data[np.abs(z_scores) > threshold]\n",
    "\n",
    "\n",
    "# Create an empty list to store the output indices from multiple columns\n",
    "index_list = []\n",
    "# Calculate the total number of rows in the DataFrame\n",
    "total_rows = len(df_imputed)\n",
    "\n",
    "for feature in ['nutrition-score-fr_100g', 'energy_100g_kcal', 'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g']:\n",
    "    # use the function\n",
    "    index_list.extend(detect_outliers_zscore(df_imputed, feature))\n",
    "    # Calculate the percentage of outliers detected\n",
    "    percentage_detected = len(index_list) / total_rows * 100\n",
    "\n",
    "    print(\n",
    "        f\"Outliers détecter dans la colonne {feature}: {len(index_list)}, Pourcentage: {percentage_detected:.2f}%\")\n",
    "\n",
    "print(\n",
    "    f\"Cumulative percentage of outliers detected: {percentage_detected:.2f}%\")\n",
    "# See rows with outliers\n",
    "# df_outliers = (df_imputed.iloc[index_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.605448162Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/3341843206.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     32\u001B[0m                      'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']\n\u001B[1;32m     33\u001B[0m num_outliers_zscore, percentage_outliers_zscore, df_outlier_z_score = detect_outliers_zscore(\n\u001B[0;32m---> 34\u001B[0;31m     df_imputed, features_to_check)\n\u001B[0m\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Number of outliers detected: {num_outliers_zscore}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "# Function for detecting outliers with Z-score Method\n",
    "def detect_outliers_zscore(data, features):\n",
    "\n",
    "    outliers = pd.DataFrame(index=data.index)\n",
    "\n",
    "    for feature in features:\n",
    "        z_scores = (data[feature] - data[feature].mean()) / data[feature].std()\n",
    "\n",
    "        # Define a z-score threshold (e.g., 3 for a significant outlier)\n",
    "        z_score_threshold = 3\n",
    "\n",
    "        # Create a mask for outliers for the current feature\n",
    "        outlier_mask = (z_scores.abs() > z_score_threshold)\n",
    "\n",
    "        # Add a new column indicating whether the feature is an outlier\n",
    "        outliers[feature + '_outlier'] = outlier_mask.astype(int)\n",
    "\n",
    "    # Sum the outlier columns to count the total number of outlier features for each row\n",
    "    outliers['total_outliers'] = outliers.sum(axis=1)\n",
    "\n",
    "    # Get the number of outliers\n",
    "    num_outliers = len(outliers[outliers['total_outliers'] > 0])\n",
    "\n",
    "    # Calculate the percentage of outliers\n",
    "    percentage_outliers = (num_outliers / len(data)) * 100\n",
    "\n",
    "    return num_outliers, percentage_outliers, outliers\n",
    "\n",
    "\n",
    "# Example usage for Z-score outlier detection:\n",
    "features_to_check = ['nutrition-score-fr_100g', 'energy_100g_kcal',\n",
    "                     'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']\n",
    "num_outliers_zscore, percentage_outliers_zscore, df_outlier_z_score = detect_outliers_zscore(\n",
    "    df_imputed, features_to_check)\n",
    "\n",
    "print(f\"Number of outliers detected: {num_outliers_zscore}\")\n",
    "print(f\"Percentage of outliers detected: {percentage_outliers_zscore:.2f}%\")\n",
    "\n",
    "# Display DataFrame with outlier information\n",
    "display(df_outlier_z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:09.720908732Z",
     "start_time": "2023-12-08T15:03:09.668729042Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/4227043438.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Concatenate df_imputed and df_outlier_z_score along columns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mcombined_z_score\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf_imputed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_outlier_z_score\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Display the combined DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcombined_z_score\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "# Concatenate df_imputed and df_outlier_z_score along columns\n",
    "combined_z_score = pd.concat([df_imputed, df_outlier_z_score], axis=1)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "display(combined_z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:09.716657847Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_62581/1570226180.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     36\u001B[0m                      'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']\n\u001B[1;32m     37\u001B[0m num_outliers_iforest, percentage_outliers_iforest, df_outlier_iforest = detect_outliers_isolation_forest(\n\u001B[0;32m---> 38\u001B[0;31m     df_imputed, features_to_check)\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m print(\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function for detecting outliers with Isolation Forest\n",
    "\n",
    "\n",
    "def detect_outliers_isolation_forest(data, features):\n",
    "    # Select the features for outlier detection\n",
    "    X = data[features].values\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "    # Fit the Isolation Forest model\n",
    "    clf = IsolationForest(contamination='auto', random_state=42)\n",
    "    y_pred = clf.fit_predict(X_standardized)\n",
    "\n",
    "    # Create a DataFrame to store the outlier information\n",
    "    outliers = pd.DataFrame(index=data.index)\n",
    "    outliers['is_outlier'] = (y_pred == -1).astype(int)\n",
    "\n",
    "    # Get the number of outliers\n",
    "    num_outliers = len(outliers[outliers['is_outlier'] == 1])\n",
    "\n",
    "    # Calculate the percentage of outliers\n",
    "    percentage_outliers = (num_outliers / len(data)) * 100\n",
    "\n",
    "    return num_outliers, percentage_outliers, outliers\n",
    "\n",
    "\n",
    "# Example usage for Isolation Forest outlier detection:\n",
    "features_to_check = ['nutrition-score-fr_100g', 'energy_100g_kcal',\n",
    "                     'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']\n",
    "num_outliers_iforest, percentage_outliers_iforest, df_outlier_iforest = detect_outliers_isolation_forest(\n",
    "    df_imputed, features_to_check)\n",
    "\n",
    "print(\n",
    "    f\"Number of outliers detected (Isolation Forest): {num_outliers_iforest}\")\n",
    "print(\n",
    "    f\"Percentage of outliers detected (Isolation Forest): {percentage_outliers_iforest:.2f}%\")\n",
    "\n",
    "# Display DataFrame with outlier information\n",
    "display(df_outlier_iforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:10.268757377Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Extract the outlier column from df_outlier_iforest\n",
    "outlier_column = 'is_outlier'\n",
    "\n",
    "# Create a scatter plot for each feature with outliers highlighted\n",
    "fig = go.Figure()\n",
    "\n",
    "for feature in features_to_check:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_imputed.index,\n",
    "        y=df_imputed[feature],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=np.where(\n",
    "                df_outlier_iforest[outlier_column] == 1, 'red', 'blue'),\n",
    "            size=8,\n",
    "            line=dict(color='black', width=1),\n",
    "        ),\n",
    "        name=feature,\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Outliers Detection with Isolation Forest',\n",
    "    xaxis=dict(title='Data Points'),\n",
    "    yaxis=dict(title='Feature Values'),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:10.269133193Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Assuming df_imputed is your DataFrame\n",
    "df_outlier_forest = df_imputed[['nutrition-score-fr_100g', 'energy_100g_kcal',\n",
    "                                'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g', 'additives_n']]\n",
    "\n",
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outlier_labels = iso_forest.fit_predict(df_outlier_forest)\n",
    "\n",
    "# Add the outlier labels to the DataFrame\n",
    "df_outlier_forest['Outlier'] = outlier_labels\n",
    "\n",
    "# Display the outliers\n",
    "outliers = df_outlier_forest[df_outlier_forest['Outlier'] == -1]\n",
    "\n",
    "# Calculate the proportion of outliers using .loc to avoid SettingWithCopyWarning\n",
    "df_outliers = df_outlier_forest.loc[df_outlier_forest['Outlier'] == -1]\n",
    "proportion_of_outliers = len(df_outliers) / len(df_outlier_forest)\n",
    "\n",
    "print(f\"Pourcentage d'outlier détecter: {proportion_of_outliers:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:03:10.274436096Z",
     "start_time": "2023-12-08T15:03:10.269329097Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_outliers(df, features, outlier_column='Outlier'):\n",
    "    # Create a scatter plot for each pair of features, colored by the outlier status\n",
    "    fig = px.scatter_matrix(df, dimensions=features, color=outlier_column)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text='Outliers Visualization using Isolation Forest',\n",
    "                      height=800)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "features_to_visualize = ['nutrition-score-fr_100g', 'energy_100g_kcal',\n",
    "                         'proteins_100g', 'sugars_100g', 'fiber_100g', 'salt_100g']\n",
    "\n",
    "visualize_outliers(df_outlier_forest, features_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changement des noms des colonnes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dataset est un dataset pour analyser les produit français, nous allons donc mettre le nom des colonnes en français.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:10.269519205Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def clean_column_names(dataframe):\n",
    "#     \"\"\"Function for set name of column in french\n",
    "\n",
    "#     Args:\n",
    "#         dataframe ([type]): [description]\n",
    "#     Returns:\n",
    "#         dataframe with the modification of columns name\n",
    "#     \"\"\"\n",
    "\n",
    "#    # Rename specific columns if needed\n",
    "\n",
    "#     column_name_mapping = {\n",
    "#         'product_name': 'nom_produit',\n",
    "#         'brands': 'marque',\n",
    "#         'countries_fr': 'pays',\n",
    "#         'nutrition_grade_fr': 'nutrition_score',\n",
    "#         'additives_n': 'nombre_additifs',\n",
    "#         'additives_fr': 'additifs',\n",
    "#         'quantity': 'quantité',\n",
    "#         'energy_100g': 'energie_100g',\n",
    "#         'proteins_100g': 'proteines_100g',\n",
    "#         'sugars_100g': 'sucres_100g',\n",
    "#         'fiber_100g': 'fibres_100g',\n",
    "#         'salt_100g': 'sel_100g'\n",
    "\n",
    "#     }\n",
    "#     dataframe.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T15:03:10.269678723Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_filtered = clean_column_names(df_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_food_act",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
